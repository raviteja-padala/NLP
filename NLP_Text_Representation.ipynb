{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPubghtp+lcRoIUu1P0y0SR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raviteja-padala/NLP/blob/main/NLP_Text_Representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Representation: Unveiling the Power of Feature Extraction from Text\n",
        "\n",
        "###  'Text representation' or 'Text Vectorization' or 'Feature extraction from text'\n",
        "\n",
        "In the realm of Natural Language Processing (NLP), the challenge lies in converting raw textual data into a form that machine learning algorithms can comprehend. This pivotal process, often referred to as \"Text Representation,\" \"Feature Extraction from Text,\" or \"Text Vectorization,\" forms the bedrock for a myriad of NLP applications.\n",
        "\n",
        "Textual data, being inherently unstructured, requires a systematic approach to unlock its insights. The goal is to transform words and sentences into numerical formats that algorithms can manipulate effectively.\n",
        "\n",
        "As we traverse this intricate landscape, armed with an array of techniques, we gain the power to extract insights, detect patterns, and unravel hidden meanings within textual data. The journey from raw text to structured numerical data reshapes how we approach NLP and harness its transformative potential."
      ],
      "metadata": {
        "id": "U0n62Vy1zo-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic terms in NLP:\n",
        "\n",
        "1. **Corpus:** An NLP corpus is a **large collection of text documents** or speech recordings that are used for language analysis. It serves as a data source for various language-related tasks in natural language processing. Corpora (plural of corpus) provide diverse examples of how words, phrases, and language patterns are used in real-world contexts, helping NLP models learn and understand language better.\n",
        "\n",
        "2. **Vocabulary:** Vocabulary refers to **the set of unique words** present in a given corpus, document, or text dataset. It's like a dictionary containing all the words used in the text. A rich vocabulary is essential for understanding and generating language accurately.\n",
        "\n",
        "3. **Document:** In NLP, a document is a single piece of text that can vary in length. A document can be as short as a single sentence or as long as an entire book. Documents are the units of analysis in text processing tasks like text classification, sentiment analysis, and topic modeling.\n",
        "\n",
        "4. **Word:** A word is a fundamental unit of language that carries meaning. It's a sequence of characters that represents a concept, object, action, or idea. Words are the building blocks of sentences and play a crucial role in conveying information and communication.\n",
        "\n",
        "In summary,\n",
        "\n",
        "* an NLP corpus is a collection of text or speech data used for analysis,\n",
        "* vocabulary is the set of unique words,\n",
        "* a document is a piece of text (short or long), and\n",
        "* a word is the basic unit of language that carries meaning.\n",
        "\n",
        "These concepts are essential for understanding and working with language in natural language processing tasks."
      ],
      "metadata": {
        "id": "sgHmE4G7D7Cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contents:\n",
        "\n",
        "- 1.One Hot Encoding\n",
        "- 2.Bag of Words (BoW)\n",
        "- 3.N-grams or Bag of N-grams\n",
        "- 4.Tf-Idf\n",
        "- 5.Custom features"
      ],
      "metadata": {
        "id": "cIFf4mQaxPL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.One Hot Encoding:\n",
        "\n",
        "One-hot encoding is a technique used in natural language processing (NLP) to represent categorical variables, such as **words as binary vectors**. Each word in a vocabulary is represented as a unique binary vector where only one element is 1 (hot) and the rest are 0 (cold). This encoding is useful for feeding categorical data into machine learning models that require numerical input.\n",
        "\n",
        "Here's an example of one-hot encoding for a small vocabulary:\n",
        "\n",
        "Suppose we have a simple vocabulary: [\"apple\", \"banana\", \"orange\", \"grape\"]\n",
        "\n",
        "1. **Word-to-Index Mapping:**\n",
        "   Each word is assigned a unique index in the vocabulary:\n",
        "   - \"apple\" -> 0\n",
        "   - \"banana\" -> 1\n",
        "   - \"orange\" -> 2\n",
        "   - \"grape\" -> 3\n",
        "\n",
        "2. **One-Hot Encoding:**\n",
        "   Each word is represented as a binary vector of the length of the vocabulary. The index corresponding to the word's position in the vocabulary is set to 1, and the rest are set to 0.\n",
        "\n",
        "   - \"apple\": [1, 0, 0, 0]\n",
        "   - \"banana\": [0, 1, 0, 0]\n",
        "   - \"orange\": [0, 0, 1, 0]\n",
        "   - \"grape\": [0, 0, 0, 1]\n",
        "\n",
        "So, in this example, the word \"apple\" is represented as [1, 0, 0, 0], \"banana\" as [0, 1, 0, 0], and so on. This binary representation preserves the categorical nature of the words and allows them to be used as input features for machine learning algorithms.\n",
        "\n",
        "However, one-hot encoding can lead to high-dimensional sparse vectors, especially for large vocabularies, which can be inefficient and computationally expensive. In practice, more advanced techniques like word embeddings (e.g., Word2Vec, GloVe) are often used to represent words in a more compact and meaningful vector space."
      ],
      "metadata": {
        "id": "6I5EV4XiNxGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros and cons of one-hot encoding:\n",
        "\n",
        "**Pros of One-Hot Encoding:**\n",
        "\n",
        "1. **Simplicity:** It's a straightforward and easy-to-understand technique for representing categorical data.\n",
        "\n",
        "2. **Interpretability:** The resulting binary vectors are interpretable, as each dimension corresponds to a specific category.\n",
        "\n",
        "3. **No Assumptions:** One-hot encoding doesn't assume any relationship or order between categories.\n",
        "\n",
        "**Cons of One-Hot Encoding:**\n",
        "\n",
        "1. **High Dimensionality:** For large vocabularies or categorical features with many levels, one-hot encoding can result in high-dimensional and **sparse data**, leading to increased memory and computation requirements.\n",
        "\n",
        "2. **Loss of Continuity:** One-hot encoding **doesn't capture any semantic relationships** between words or categories. All categories are treated as independent.\n",
        "\n",
        "3. **Curse of Dimensionality:** High-dimensional data can suffer from the \"curse of dimensionality,\" where distances between data points become less meaningful, and the risk of overfitting increases.\n",
        "\n",
        "4. **Not Suitable for Text Sequences:** One-hot encoding doesn't capture the sequential nature of words in text. It treats each word as independent, ignoring the contextual information.\n",
        "\n",
        "\n",
        "In summary, while one-hot encoding is simple and intuitive for representing categorical data, it can lead to high-dimensional data and lack of context in certain NLP applications. More advanced techniques like word embeddings and categorical encodings have been developed to address some of these limitations."
      ],
      "metadata": {
        "id": "Q4Yj6NnPN-XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "t5XAIS4pPoMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Bag of Words\" (BoW) is a basic and widely used technique in natural language processing (NLP) for representing text data numerically. It's a simple model that **focuses on the frequency of words in a document**, ignoring the order and structure of the words. The name \"Bag of Words\" reflects the idea that you're treating a document as a bag that contains all the words in it, disregarding their sequence.\n",
        "\n",
        "Here's how the Bag of Words model works:\n",
        "\n",
        "1. **Tokenization:** Break down a document into individual words or tokens.\n",
        "\n",
        "2. **Vocabulary Creation:** Create a vocabulary containing all unique words from the entire corpus. Each word is assigned a unique index.\n",
        "\n",
        "3. **Counting:** For each document, count how many times each word from the vocabulary appears in that document. This information is stored in a frequency vector.\n",
        "\n",
        "4. **Vectorization:** Represent each document as a vector where the value at each index corresponds to the frequency of the word at that index in the document.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "Suppose you have two documents:\n",
        "1. \"The cat chased the mouse.\"\n",
        "2. \"The mouse ran away.\"\n",
        "\n",
        "**Vocabulary:** [\"The\", \"cat\", \"chased\", \"mouse\", \"ran\", \"away\"]\n",
        "\n",
        "**Bag of Words Representation:**\n",
        "1. [2, 1, 1, 1, 0, 0]\n",
        "2. [1, 0, 0, 1, 1, 1]\n",
        "\n",
        "In the first document, \"The\" appears twice, \"cat,\" \"chased,\" and \"mouse\" appear once each, and the other words don't appear. So, the vector [2, 1, 1, 1, 0, 0] represents the first document's bag of words.\n",
        "\n",
        "In the second document, \"The,\" \"mouse,\" \"ran,\" and \"away\" appear once each, and the other words don't appear. So, the vector [1, 0, 0, 1, 1, 1] represents the second document's bag of words.\n",
        "\n",
        "BoW is simple and effective for tasks like text classification and sentiment analysis. However, it doesn't consider word order, grammar, or semantic meaning. More advanced techniques like TF-IDF (Term Frequency-Inverse Document Frequency) and word embeddings address some of these limitations by capturing more nuanced information from the text data."
      ],
      "metadata": {
        "id": "GjH4pLW7Ps2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The cat chased the mouse.\",\n",
        "    \"The mouse ran away.\",\n",
        "    \"The cat and the mouse are friends.\"\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the documents into a bag of words representation\n",
        "BOW = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the vocabulary (unique words in the documents)\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the bag of words matrix to an array\n",
        "X_array = BOW.toarray()\n",
        "\n",
        "# Print the vocabulary and the bag of words representation\n",
        "print(\"Alphabetical ordering of words\", vectorizer.vocabulary_)\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "\n",
        "print(\"Bag of Words Representation:\\n\", X_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2BblVw3Ptn6",
        "outputId": "6a0d5806-024a-4d39-b487-c55b7c8c2d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alphabetical ordering of words {'the': 8, 'cat': 3, 'chased': 4, 'mouse': 6, 'ran': 7, 'away': 2, 'and': 0, 'are': 1, 'friends': 5}\n",
            "Vocabulary: ['and' 'are' 'away' 'cat' 'chased' 'friends' 'mouse' 'ran' 'the']\n",
            "Bag of Words Representation:\n",
            " [[0 0 0 1 1 0 1 0 2]\n",
            " [0 0 1 0 0 0 1 1 1]\n",
            " [1 1 0 1 0 1 1 0 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CountVectorizer` is a class in scikit-learn that is used for converting a collection of text documents into a matrix of token counts. It has several hyperparameters that allow you to customize its behavior. Here are some important hyperparameters of `CountVectorizer`:\n",
        "\n",
        "1. **`analyzer`**: Specifies whether to tokenize the input text at the word level or character level. It takes values like `'word'` (default) or `'char'`.\n",
        "\n",
        "2. **`tokenizer`**: Allows you to specify a custom function for tokenizing the input text. If not specified, the `CountVectorizer` will use its default tokenizer.\n",
        "\n",
        "3. **`stop_words`**: Specifies a list of stop words that will be ignored during tokenization. It can take values like `'english'` (for using the built-in English stop words) or a list of custom stop words.\n",
        "\n",
        "4. **`ngram_range`**: Specifies the range of n-grams to be extracted. An n-gram is a sequence of n words. For example, setting `ngram_range=(1, 2)` would include both single words and pairs of consecutive words (bigrams).\n",
        "\n",
        "5. **`max_df`**: Specifies the threshold for excluding words that appear in a certain percentage of documents. For instance, if `max_df=0.8`, words appearing in 80% or more of the documents will be excluded.\n",
        "\n",
        "6. **`min_df`**: Specifies the threshold for excluding words that appear in a certain number or percentage of documents. For example, `min_df=2` would exclude words appearing in only one document.\n",
        "\n",
        "7. **`max_features`**: Limits the vocabulary size to the specified number of most frequent terms. This can help manage memory and computation requirements.\n",
        "\n",
        "8. **`lowercase`**: Controls whether the text should be converted to lowercase before tokenization. Defaults to `True`.\n",
        "\n",
        "9. **`preprocessor`**: Allows you to specify a custom function to preprocess the text before tokenization.\n",
        "\n",
        "10. **`Binary`** The binary hyperparameter in CountVectorizer controls whether to use binary or count features. If binary=True, then each feature will be represented by a single binary value, indicating whether the feature is present or absent in the document. If binary=False, then each feature will be represented by its count in the document.\n",
        "* The **default value of binary is False**. This means that CountVectorizer will use count features by default. However, you can set binary=True if you want to use binary features instead. Used in sentiment analysis mostly\n"
      ],
      "metadata": {
        "id": "EybQ2u07VeZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparametrs\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"The cat chased the mouse.\",\n",
        "    \"The mouse ran away.\",\n",
        "    \"The cat and the mouse are friends.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase=True,       # to convert text to lowercase,\n",
        "    stop_words='english', # remove English stop words\n",
        "    ngram_range=(1, 2),   # extract both single words and bigrams\n",
        "    max_df=0.8,           # exclude words appearing in more than 80% of documents\n",
        "    min_df=1,             # include words appearing in at least one document\n",
        "    max_features=None     # not limit the vocabulary size\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the bag of words matrix to an array\n",
        "X_array = X.toarray()\n",
        "print(\"Alphabetical ordering of words\", vectorizer.vocabulary_)\n",
        "print('\\n')\n",
        "print(\"Bag of Words Representation:\\n\", X_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbl_iQopPtrF",
        "outputId": "fc206688-3956-4324-ea89-81ec6186caed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alphabetical ordering of words {'cat': 1, 'chased': 4, 'cat chased': 2, 'chased mouse': 5, 'ran': 9, 'away': 0, 'mouse ran': 8, 'ran away': 10, 'friends': 6, 'cat mouse': 3, 'mouse friends': 7}\n",
            "\n",
            "\n",
            "Bag of Words Representation:\n",
            " [[0 1 1 0 1 1 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 1 1 1]\n",
            " [0 1 0 1 0 0 1 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros and cons of the Bag of Words (BoW) model in natural language processing:\n",
        "\n",
        "**Pros of Bag of Words (BoW):**\n",
        "\n",
        "1. **Simplicity:** BoW is a straightforward and simple representation for text data.\n",
        "\n",
        "2. **Ease of Implementation:** Implementing BoW is relatively easy, making it a good starting point for text-based projects.\n",
        "\n",
        "3. **Feature Extraction:** BoW converts text data into a numerical format that can be used with various machine learning algorithms.\n",
        "\n",
        "4. **Language Independence:** BoW treats words as independent units, making it suitable for languages without complex grammar structures.\n",
        "\n",
        "5. **Useful for Shallow Models:** BoW can work well with simpler models like Naive Bayes and Logistic Regression.\n",
        "\n",
        "6. **Interpretability:** The resulting features (word frequencies) are interpretable and can provide insights into text data.\n",
        "\n",
        "**Cons of Bag of Words (BoW):**\n",
        "\n",
        "1. **Lack of Context:** BoW completely **ignores word order** and context, losing valuable information in text sequences.\n",
        "\n",
        "2. **Sparse Representation:** BoW results in high-dimensional **sparse** vectors, which can lead to memory and computation issues.\n",
        "\n",
        "3. **Loss of Semantic Meaning:** BoW treats words with multiple meanings the same, lacking the ability to capture **semantic relationships**.\n",
        "\n",
        "4. **No Sequence Information:** BoW fails to capture sequential information, which is important in many NLP tasks.\n",
        "\n",
        "5. **Sensitive to Vocabulary Size:** The choice of vocabulary size affects the representation's quality and model performance.\n",
        "\n",
        "6. **Out-of-Vocabulary Issue:** BoW struggles with handling words it hasn't seen during training (OOV words).\n",
        "\n",
        "7. **Limited by Frequency:** BoW focuses solely on word frequency, disregarding other important aspects like word importance.\n",
        "\n",
        "8. **Not Effective for Deep Learning:** Modern deep learning models benefit from understanding sequential relationships and nuances, which BoW doesn't capture.\n",
        "\n",
        "In summary, while Bag of Words is a simple and effective technique for text representation, it has limitations related to context, sparsity, and lack of semantic understanding, making it less suitable for more advanced NLP tasks."
      ],
      "metadata": {
        "id": "smVGb85ocChZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\"This movie is very good\"]\n",
        "\n",
        "documents = [\n",
        "    \"This movie is very good\",\n",
        "    \"This movie is not very good\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase=True,       # to convert text to lowercase,\n",
        "    stop_words='english', # remove English stop words\n",
        "    ngram_range=(1, 2),   # extract both single words and bigrams\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the bag of words matrix to an array\n",
        "X_array = X.toarray()\n",
        "print(\"Alphabetical ordering of words\", vectorizer.vocabulary_)\n",
        "print('\\n')\n",
        "print(\"Bag of Words Representation:\\n\", X_array)\n",
        "\n",
        "\n",
        "# both sentences have different meaning\n",
        "# but Bag of words vectorised them into similar words, this is one of flaw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ragy2sZngLbR",
        "outputId": "a0480377-8375-4497-9bab-71cf4e15026a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alphabetical ordering of words {'movie': 1, 'good': 0, 'movie good': 2}\n",
            "\n",
            "\n",
            "Bag of Words Representation:\n",
            " [[1 1 1]\n",
            " [1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.N-grams or Bag of N-grams"
      ],
      "metadata": {
        "id": "vflM0jUJcd_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In natural language processing (NLP), **n-grams** are contiguous sequences of 'n' items from a given sample of text or speech. The items can be words, characters, or even phonemes, depending on the context. N-grams are used to capture local patterns and dependencies within a sequence of tokens. They're especially useful for tasks involving context, like language modeling, machine translation, and text generation.\n",
        "\n",
        "Let's look at an example to understand n-grams better. Consider the sentence:\n",
        "\n",
        "**\"The quick brown fox jumps over the lazy dog.\"**\n",
        "\n",
        "For different values of 'n', here are the n-grams:\n",
        "\n",
        "- **Unigrams (1-grams):** [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog.\"]\n",
        "- **Bigrams (2-grams):** [\"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", \"the lazy\", \"lazy dog.\"]\n",
        "- **Trigrams (3-grams):** [\"The quick brown\", \"quick brown fox\", \"brown fox jumps\", \"fox jumps over\", \"jumps over the\", \"over the lazy\", \"the lazy dog.\"]\n",
        "- **4-grams:** [\"The quick brown fox\", \"quick brown fox jumps\", \"brown fox jumps over\", \"fox jumps over the\", \"jumps over the lazy\", \"over the lazy dog.\"]\n",
        "\n",
        "N-grams are valuable in NLP tasks as they capture local patterns and dependencies, enabling models to better understand and generate text with context."
      ],
      "metadata": {
        "id": "a9HuRL2BciX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dJjwHtxddNQ",
        "outputId": "750d9578-285b-48e0-8a70-4a9a9a2bd14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Python example of extracting n-grams using the `nltk` library\n",
        "# In this example, the sentence is tokenized into words, and then n-grams of different sizes are generated using the `ngrams` function from `nltk.util`.\n",
        "# The output shows the extracted n-grams for each value of 'n'.\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Generate n-grams for different values of n\n",
        "unigrams = list(ngrams(words, 1))\n",
        "bigrams = list(ngrams(words, 2))\n",
        "trigrams = list(ngrams(words, 3))\n",
        "fourgrams = list(ngrams(words, 4))\n",
        "\n",
        "print(\"Unigrams:\", unigrams)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)\n",
        "print(\"4-grams:\", fourgrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLcsTIM9Pttv",
        "outputId": "eaf2cc8f-fb2c-430e-eed6-161b29460367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams: [('The',), ('quick',), ('brown',), ('fox',), ('jumps',), ('over',), ('the',), ('lazy',), ('dog',), ('.',)]\n",
            "Bigrams: [('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog'), ('dog', '.')]\n",
            "Trigrams: [('The', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog'), ('lazy', 'dog', '.')]\n",
            "4-grams: [('The', 'quick', 'brown', 'fox'), ('quick', 'brown', 'fox', 'jumps'), ('brown', 'fox', 'jumps', 'over'), ('fox', 'jumps', 'over', 'the'), ('jumps', 'over', 'the', 'lazy'), ('over', 'the', 'lazy', 'dog'), ('the', 'lazy', 'dog', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of generating n-grams using libraries like NLTK or scikit-learn, there aren't many hyperparameters to consider, as generating n-grams is a relatively straightforward process. However, there are some aspects to customize based on specific needs:\n",
        "\n",
        "**Value of 'n'**: This is the most important parameter and determines the size of the n-grams. You need to decide whether you want unigrams (single tokens), bigrams (pairs of tokens), trigrams (triplets of tokens), and so on.\n",
        "\n",
        "**Tokenization**: Before generating n-grams, you need to tokenize your input text into tokens (words or characters). Depending on your requirements, you might want to consider different tokenization strategies.\n",
        "\n",
        "**Padding and Truncation**: If your text data includes padding or you want to truncate it, you might need to adjust the tokenization process or deal with partial n-grams."
      ],
      "metadata": {
        "id": "L3IuSOmreIqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "\n",
        "sentence = \"This movie is very good\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Generate n-grams for different values of n\n",
        "unigrams = list(ngrams(words, 1))\n",
        "bigrams = list(ngrams(words, 2))\n",
        "trigrams = list(ngrams(words, 3))\n",
        "fourgrams = list(ngrams(words, 4))\n",
        "\n",
        "print(\"Unigrams:\", unigrams)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)\n",
        "print(\"4-grams:\", fourgrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKb-9Wjoep0X",
        "outputId": "c371d293-05bb-4afc-b80e-26815a14f6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams: [('This',), ('movie',), ('is',), ('very',), ('good',)]\n",
            "Bigrams: [('This', 'movie'), ('movie', 'is'), ('is', 'very'), ('very', 'good')]\n",
            "Trigrams: [('This', 'movie', 'is'), ('movie', 'is', 'very'), ('is', 'very', 'good')]\n",
            "4-grams: [('This', 'movie', 'is', 'very'), ('movie', 'is', 'very', 'good')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\"This movie is very good\"]\n",
        "\n",
        "documents = [\n",
        "    \"This movie is very good\",\n",
        "    \"This movie is not very good\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase=True,       # to convert text to lowercase,\n",
        "    stop_words='english', # remove English stop words\n",
        "    ngram_range=(1, 2),   # extract both single words and bigrams\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the bag of words matrix to an array\n",
        "X_array = X.toarray()\n",
        "print(\"Alphabetical ordering of words\", vectorizer.vocabulary_)\n",
        "print('\\n')\n",
        "print(\"Bag of Words Representation:\\n\", X_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsUDrbuHe1UP",
        "outputId": "de708cef-b571-4528-e3cc-763fecf8b93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alphabetical ordering of words {'movie': 1, 'good': 0, 'movie good': 2}\n",
            "\n",
            "\n",
            "Bag of Words Representation:\n",
            " [[1 1 1]\n",
            " [1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"This movie is very good\",\n",
        "    \"This movie is not very good\"\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer instance for unigrams\n",
        "vectorizer_unigrams = CountVectorizer(ngram_range=(1, 1))\n",
        "X_unigrams = vectorizer_unigrams.fit_transform(documents)\n",
        "\n",
        "# Create a CountVectorizer instance for bigrams\n",
        "vectorizer_bigrams = CountVectorizer(ngram_range=(2, 2))\n",
        "X_bigrams = vectorizer_bigrams.fit_transform(documents)\n",
        "\n",
        "# Get the vocabulary for unigrams and bigrams\n",
        "vocabulary_unigrams = vectorizer_unigrams.get_feature_names_out()\n",
        "vocabulary_bigrams = vectorizer_bigrams.get_feature_names_out()\n",
        "\n",
        "# Convert the count matrices to arrays\n",
        "X_unigrams_array = X_unigrams.toarray()\n",
        "X_bigrams_array = X_bigrams.toarray()\n",
        "\n",
        "# Print vocabulary and count matrices for unigrams and bigrams\n",
        "print(\"Vocabulary (Unigrams):\", vocabulary_unigrams)\n",
        "print(\"Count Matrix (Unigrams):\\n\", X_unigrams_array)\n",
        "\n",
        "print(\"\\nVocabulary (Bigrams):\", vocabulary_bigrams)\n",
        "print(\"Count Matrix (Bigrams):\\n\", X_bigrams_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cokHMUqPe6G0",
        "outputId": "2d23adeb-2ada-482a-c8f2-e0b0008e40e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Unigrams): ['good' 'is' 'movie' 'not' 'this' 'very']\n",
            "Count Matrix (Unigrams):\n",
            " [[1 1 1 0 1 1]\n",
            " [1 1 1 1 1 1]]\n",
            "\n",
            "Vocabulary (Bigrams): ['is not' 'is very' 'movie is' 'not very' 'this movie' 'very good']\n",
            "Count Matrix (Bigrams):\n",
            " [[0 1 1 0 1 1]\n",
            " [1 0 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "\n",
        "In the unigrams representation, the generated 6-dimensional vectors are similar in 5 dimensions, keeping the vectors close to each other. However, the meaning of the sentences is very different. The BoW model, based solely on word frequencies, doesn't capture the negation \"not\" in the second document, leading to a misleading similarity in the vectors.\n",
        "\n",
        "On the other hand, in the bigrams representation, the generated vectors are not very similar, accurately capturing the meaning of the sentences. The bigrams approach considers pairs of consecutive words, allowing it to capture important semantic distinctions that were missed by the unigrams representation.\n",
        "\n",
        "In summary, while unigrams might not adequately capture certain nuances like negations, bigrams provide a more meaningful representation by considering word pairs and preserving context. This example showcases the importance of choosing an appropriate n-gram size based on the specific language patterns and context in the dataset."
      ],
      "metadata": {
        "id": "JfKx1e1giBGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A concise summary of the pros and cons of using n-grams in natural language processing:\n",
        "\n",
        "**Pros of N-grams:**\n",
        "\n",
        "1. **Contextual Information:** N-grams capture local patterns and dependencies within a sequence of tokens, allowing models to understand context better.\n",
        "\n",
        "2. **Language Patterns:** N-grams help in capturing common phrases, collocations, and idiomatic expressions that might be missed by single words.\n",
        "\n",
        "3. **Simplicity:** Generating n-grams is relatively simple and doesn't require complex linguistic analysis.\n",
        "\n",
        "4. **Useful for Shallow Models:** N-grams can work effectively with simpler models like Naive Bayes and Linear Regression.\n",
        "\n",
        "5. **Improved Feature Space:** N-grams provide a way to include some linguistic structure without the complexity of full syntactic parsing.\n",
        "\n",
        "6. **Reduced Dimensionality:** Compared to full syntactic or semantic analysis, n-grams offer a compromise between capturing information and keeping dimensionality manageable.\n",
        "\n",
        "**Cons of N-grams:**\n",
        "\n",
        "1. **Sparsity:** N-grams can lead to high-dimensional sparse data, especially when considering higher-order n-grams, which might pose challenges in computation and memory.\n",
        "\n",
        "2. **Limited Context:** While n-grams capture local context, they might not capture long-range dependencies or global context effectively.\n",
        "\n",
        "3. **Data Size Impact:** Longer n-grams can be impacted by data size limitations; you need sufficient data to estimate the frequency of all possible n-grams.\n",
        "\n",
        "4. **Lack of Semantics:** N-grams might not capture semantic relationships between words, as they focus on co-occurrence patterns.\n",
        "\n",
        "5. **Out-of-Vocabulary Words:** N-grams can struggle with out-of-vocabulary (OOV) words and unseen combinations, particularly in languages with complex word forms or new terms.\n",
        "\n",
        "6. **Order Sensitivity:** N-grams are sensitive to the order of words, which can be a limitation when the same concepts are expressed in different word orders.\n",
        "\n",
        "7. **Data Sparsity for Higher N-grams:** Higher-order n-grams (e.g., 4-grams) suffer from data sparsity, as each specific combination might occur less frequently.\n",
        "\n",
        "8. **Language-Dependent:** The effectiveness of n-grams can vary across languages due to differences in word order and linguistic structures.\n",
        "\n",
        "In summary, n-grams offer a balance between context capture and simplicity, making them valuable for various NLP tasks. However, they also come with challenges related to sparsity, context limitations, and their inability to capture deeper semantic relationships. The choice of n-gram size should be based on the specific language patterns and tasks you're dealing with."
      ],
      "metadata": {
        "id": "DcKfQqpHkZli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Tf-Idf (Term Frequency-Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "47ywvGHOk5N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF  is a numerical representation used in natural language processing to evaluate the **importance of a word within a document relative to its occurrence in a collection of documents (corpus).** It takes into account both the frequency of the word in the document (TF) and how unique the word is across the entire corpus (IDF).\n",
        "\n",
        "Here's how TF-IDF works:\n",
        "\n",
        "**Term Frequency** (TF): This calculates how often a word appears in a document relative to the total number of words in that document.\n",
        "\n",
        "**TF** = (Number of times the word appears in the document) / (Total number of words in the document)\n",
        "\n",
        "**Inverse Document Frequency**(IDF): This measures the uniqueness of a word by considering how many documents in the corpus contain that word. It's calculated as the logarithm of the total number of documents divided by the number of documents containing the word.\n",
        "\n",
        "**IDF** = log((Total number of documents) / (Number of documents containing the word))\n",
        "\n",
        "TF-IDF Score: The TF-IDF score for a word in a document is the product of its TF and IDF values.\n",
        "\n",
        "**TF-IDF** = TF * IDF\n",
        "\n",
        "- Higher TF-IDF values indicate that a word is important in a particular document and relatively unique across the corpus.\n",
        "\n",
        "\n",
        "- TF-IDF is commonly used in information retrieval, text classification, clustering, and other NLP tasks where word importance and uniqueness matter."
      ],
      "metadata": {
        "id": "B9j6KUJklTMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The cat chased the mouse.\",\n",
        "    \"The mouse ran away.\",\n",
        "    \"The cat and the mouse are friends.\"\n",
        "]\n",
        "\n",
        "# Create a TfidfVectorizer instance\n",
        "tfidf_vectorizer = TfidfVectorizer() #TfidfVectorizer is used to convert the text documents into a TF-IDF matrix.\n",
        "\n",
        "# Fit and transform the documents into a TF-IDF matrix\n",
        "X = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (words) from the vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to an array\n",
        "X_array = X.toarray()\n",
        "\n",
        "\n",
        "# Print the feature names and TF-IDF matrix\n",
        "print(\"Feature Names (Words):\", feature_names)\n",
        "print('\\n')\n",
        "print(\"TF-IDF Matrix:\\n\", X_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcSHME4ge6Jr",
        "outputId": "7aabb078-8dfd-41d1-d906-289e52792a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Words): ['and' 'are' 'away' 'cat' 'chased' 'friends' 'mouse' 'ran' 'the']\n",
            "\n",
            "\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.4172334  0.54861178 0.\n",
            "  0.32401895 0.         0.64803791]\n",
            " [0.         0.         0.6088451  0.         0.         0.\n",
            "  0.35959372 0.6088451  0.35959372]\n",
            " [0.43345167 0.43345167 0.         0.32965117 0.         0.43345167\n",
            "  0.25600354 0.         0.51200708]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the IDF values\n",
        "idf_values = tfidf_vectorizer.idf_\n",
        "\n",
        "# Print words and their corresponding TF-IDF values\n",
        "for word, idf_value in zip(feature_names, idf_values):\n",
        "    print(f\"{word}: {idf_value:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWvWWhfXe6MM",
        "outputId": "07a932cd-cd02-4e01-ebf3-31d5a71448f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and: 1.693147\n",
            "are: 1.693147\n",
            "away: 1.693147\n",
            "cat: 1.287682\n",
            "chased: 1.693147\n",
            "friends: 1.693147\n",
            "mouse: 1.000000\n",
            "ran: 1.693147\n",
            "the: 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason we're seeing differences between the TF-IDF matrix values and the printed IDF values is because of how TF-IDF is calculated and how the values are presented in the output.\n",
        "\n",
        "In the TF-IDF matrix, the values are not only influenced by the IDF component but also by the Term Frequency (TF) component. The TF component represents how often a word occurs in a specific document. The TF-IDF score for a word in a specific document is the product of its TF and IDF values.\n",
        "\n",
        "Here's a breakdown of what we're seeing:\n",
        "\n",
        "1. **TF-IDF Matrix:**\n",
        "   The values in the matrix are the actual TF-IDF scores for each word in each document. Each cell represents the TF-IDF score of a word in a particular document.\n",
        "\n",
        "2. **Printed IDF Values:**\n",
        "   The printed IDF values are the IDF components of the TF-IDF scores. They represent the uniqueness of each word across the entire corpus. These values are not directly multiplied with the term frequencies (TF) to get the TF-IDF scores.\n",
        "\n",
        "In the TF-IDF matrix, we see how both TF and IDF contribute to the final TF-IDF scores. The IDF values themselves are just a part of the calculation. The printed IDF values are presented separately to show the uniqueness of each word in the corpus, but they aren't used directly to compute the values in the TF-IDF matrix."
      ],
      "metadata": {
        "id": "DJfmTPwhqvBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why is log used when calculating term frequency weight TF and IDF, inverse document frequency?\n",
        "\n",
        "The formula for IDF is log( N / df t ) instead of just N / df t. Where N = total documents in collection, and df t = document frequency of term t. Log is said to be used because it “dampens” the effect of IDF.\n",
        "\n",
        "The use of the logarithm in calculating the Term Frequency-Inverse Document Frequency (TF-IDF) weight and Inverse Document Frequency (IDF) is a mathematical choice that helps in addressing certain issues related to scaling and the way information is distributed in natural language data.\n",
        "\n",
        "**Logarithmic Scaling:**\n",
        "\n",
        "1. **Term Frequency (TF):** The purpose of taking the logarithm of the term frequency is to dampen the effect of large term frequencies. Without logarithmic scaling, a single document with a very high term frequency for a particular word could disproportionately dominate the TF-IDF score, making it less representative of the word's importance across the entire corpus.\n",
        "\n",
        "2. **Inverse Document Frequency (IDF):** The logarithmic scaling of IDF smoothens the impact of the IDF values. It reduces the influence of extremely rare words with very high IDF values, making the IDF values more stable and balanced across different words.\n",
        "\n",
        "**Data Distribution:**\n",
        "\n",
        "In natural language data, word frequencies often follow a power-law distribution, where a small number of words occur very frequently (e.g., \"the\", \"and\") while the majority of words are relatively rare. This distribution can lead to skewed TF and IDF values. Applying a logarithmic transformation helps in normalizing the distribution of scores.\n",
        "\n",
        "**Balance and Interpretability:**\n",
        "\n",
        "Using logarithms helps to balance the TF-IDF values and make them more interpretable. It prevents large values from dominating and provides a better representation of word importance without introducing biases due to outliers.\n",
        "\n",
        "Overall, the use of logarithms in TF and IDF calculations is a common practice in TF-IDF to ensure that the resulting scores are meaningful, stable, and representative of word importance in a corpus. It addresses challenges posed by the distribution of word frequencies and helps create a more balanced and interpretable representation of text data."
      ],
      "metadata": {
        "id": "alkr1uhErpC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Pros of TF-IDF:**\n",
        "\n",
        "1. **Word Importance:** TF-IDF captures the relative importance of words in a document compared to their occurrence in the entire corpus, aiding in content analysis.\n",
        "\n",
        "2. **Flexibility:** TF-IDF is language-agnostic and applicable to various text analysis tasks, such as information retrieval, text classification, and clustering.\n",
        "\n",
        "3. **Simple to Implement:** Implementing TF-IDF is relatively simple, making it accessible to beginners in NLP.\n",
        "\n",
        "4. **Document Comparison:** TF-IDF enables efficient comparison of documents based on content similarity, supporting tasks like document clustering and recommendation systems.\n",
        "\n",
        "5. **Handles Stop Words:** TF-IDF naturally downweights common words (stop words) that appear frequently in many documents but provide less discriminatory power.\n",
        "\n",
        "6. **Customization:** You can fine-tune TF-IDF by adjusting parameters like term frequency normalization and IDF smoothing.\n",
        "\n",
        "7. **Interpretability:** TF-IDF scores offer some interpretability, allowing you to assess the significance of words in documents.\n",
        "\n",
        "**Cons of TF-IDF:**\n",
        "\n",
        "1. **Lack of Semantic Understanding:** TF-IDF treats words as independent entities, disregarding semantic relationships and context.\n",
        "\n",
        "2. **Sparse Representations:** TF-IDF matrices can be high-dimensional and sparse, which might lead to memory and computation challenges.\n",
        "\n",
        "3. **Out-of-Vocabulary Words:** New or unseen words during testing might receive zero IDF values, causing issues when calculating TF-IDF.\n",
        "\n",
        "4. **Normalization Issues:** Different documents can have varying lengths, affecting term frequency normalization and influencing TF-IDF scores.\n",
        "\n",
        "5. **Domain Sensitivity:** IDF values can be influenced by domain-specific terms, potentially leading to inappropriate ranking in certain contexts.\n",
        "\n",
        "6. **Not Sequence-Aware:** TF-IDF doesn't consider word order, which is essential for tasks like sentiment analysis or text generation.\n",
        "\n",
        "7. **Rare Term Impact:** Extremely rare terms might have disproportionately high IDF values, affecting the final TF-IDF scores.\n",
        "\n",
        "8. **Choice of Parameters:** The choice of parameters like term frequency normalization and IDF smoothing can affect the results and requires domain expertise.\n",
        "\n",
        "In summary, TF-IDF is a versatile and effective method for representing and analyzing text data, but it has limitations related to semantics, sparsity, and sequence awareness. It's important to consider your specific task, data, and goals when deciding whether to use TF-IDF or other methods in your NLP projects."
      ],
      "metadata": {
        "id": "Chu5sacnsWzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Custom features in natural language processing (NLP)"
      ],
      "metadata": {
        "id": "X5lFX1_StW-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom features in natural language processing (NLP) refer to manually designed attributes or characteristics that are extracted from text data and used as input features for machine learning models. These features go beyond simple word frequencies and include engineered elements that are tailored to the specific task at hand. Custom features can enhance the performance of models by providing additional context, domain knowledge, or linguistic insights.\n",
        "\n",
        "Let's consider an example of sentiment analysis, where the goal is to classify text as positive or negative sentiment. In addition to standard features like TF-IDF or word embeddings, we can create custom features based on linguistic cues.\n",
        "\n",
        "---\n",
        "\n",
        "*Example: Custom Features for Sentiment Analysis*\n",
        "\n",
        "Suppose we want to perform sentiment analysis on product reviews. We can create the following custom features:\n",
        "\n",
        "*Exclamation Mark Count*: Some studies show that the use of exclamation marks might indicate positive sentiment. We can count the number of exclamation marks in each review.\n",
        "\n",
        "*Capitalization Ratio*: Uppercase words might convey strong emotions. We can calculate the ratio of capitalized words to the total number of words in a review.\n",
        "\n",
        "*Emoticon Presence:* Emoticons like :) and :( can provide valuable sentiment information. We can check if certain emoticons appear in the review.\n",
        "\n",
        "*Positive/Negative Keywords:* Create lists of positive and negative words relevant to the product domain. Count the occurrences of these words in the review.\n",
        "\n",
        "*Sentence Length*: Longer sentences might indicate more elaborate opinions. We can include the average sentence length in a review.\n",
        "\n"
      ],
      "metadata": {
        "id": "dJjXE5fItbbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Sample reviews\n",
        "reviews = [\n",
        "    \"This product is amazing! I love it.\",\n",
        "    \"Not satisfied with the quality. Very disappointed.\"\n",
        "]\n",
        "\n",
        "data = {'review': reviews}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Custom feature extraction\n",
        "df['exclamation_count'] = df['review'].apply(lambda x: x.count('!'))\n",
        "df['capitalization_ratio'] = df['review'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x.split()))\n",
        "df['positive_emoticon'] = df['review'].apply(lambda x: 1 if ':)' in x else 0)\n",
        "df['negative_emoticon'] = df['review'].apply(lambda x: 1 if ':(' in x else 0)\n",
        "\n",
        "# List of positive/negative keywords\n",
        "positive_keywords = ['amazing', 'love']\n",
        "negative_keywords = ['not satisfied', 'disappointed']\n",
        "\n",
        "df['positive_keyword_count'] = df['review'].apply(lambda x: sum(1 for word in positive_keywords if word in x))\n",
        "df['negative_keyword_count'] = df['review'].apply(lambda x: sum(1 for word in negative_keywords if word in x))\n",
        "\n",
        "# Average sentence length\n",
        "df['avg_sentence_length'] = df['review'].apply(lambda x: len(x.split()) / len(TextBlob(x).sentences))\n",
        "\n",
        "#custom features\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "TzYeaCkOe6Rm",
        "outputId": "2ddc8a72-e9e6-4447-eb6e-50a582826af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  exclamation_count  \\\n",
              "0                This product is amazing! I love it.                  1   \n",
              "1  Not satisfied with the quality. Very disappoin...                  0   \n",
              "\n",
              "   capitalization_ratio  positive_emoticon  negative_emoticon  \\\n",
              "0              0.285714                  0                  0   \n",
              "1              0.285714                  0                  0   \n",
              "\n",
              "   positive_keyword_count  negative_keyword_count  avg_sentence_length  \n",
              "0                       2                       0                  3.5  \n",
              "1                       0                       1                  3.5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff9fcaef-b06b-498f-8d6e-9141601b9d41\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>exclamation_count</th>\n",
              "      <th>capitalization_ratio</th>\n",
              "      <th>positive_emoticon</th>\n",
              "      <th>negative_emoticon</th>\n",
              "      <th>positive_keyword_count</th>\n",
              "      <th>negative_keyword_count</th>\n",
              "      <th>avg_sentence_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This product is amazing! I love it.</td>\n",
              "      <td>1</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not satisfied with the quality. Very disappoin...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff9fcaef-b06b-498f-8d6e-9141601b9d41')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ff9fcaef-b06b-498f-8d6e-9141601b9d41 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ff9fcaef-b06b-498f-8d6e-9141601b9d41');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:\n",
        "\n",
        "In this exploration, we delved into key methods for transforming text data into numerical representations suitable for machine learning tasks.\n",
        "\n",
        "By understanding the importance and role of these feature extraction techniques, we gain the ability to transform raw text into structured numerical data ready for machine learning algorithms. However, it's crucial to keep in mind that each method has its pros and cons:\n",
        "\n",
        "- **One Hot Encoding:** Simple and intuitive, but inefficient for large vocabularies.\n",
        "- **BoW:** Efficient and useful for basic analysis, but loses word order and nuances.\n",
        "- **N-grams:** Captures partial word order and context, but increases dimensionality.\n",
        "- **Tf-Idf:** Considers term importance and document context, but might not fully capture semantics.\n",
        "- **Custom Features:** Tailored to specific tasks, but requires domain expertise and additional effort.\n",
        "\n",
        "With this understanding, we can make informed decisions about which technique to apply based on the specific goals of our NLP tasks. By leveraging these techniques and their respective strengths, we can empower our models to extract meaningful insights from textual data and improve the overall performance of NLP applications."
      ],
      "metadata": {
        "id": "tQV3EBBfPrqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thank you for reading till the end"
      ],
      "metadata": {
        "id": "wG7r6uN3zGkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Raviteja\n",
        "https://www.linkedin.com/in/raviteja-padala/"
      ],
      "metadata": {
        "id": "iMUdg4_9zLAx"
      }
    }
  ]
}