{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/Ji1QprL/4grAtCk9l2Xc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raviteja-padala/NLP/blob/main/Dirichlet_distribution_Application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dirichlet distribution - Application"
      ],
      "metadata": {
        "id": "nVlNfRX-JsI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objective:\n",
        "\n",
        "To gain a comprehensive understanding of the Dirichlet distribution and its practical applications, with a specific focus on its role in Latent Dirichlet Allocation (LDA) for topic modeling. Through real-world examples and explanations, explore how the Dirichlet distribution shapes probability distributions, influences decision-making processes, and contributes to the extraction of meaningful topics from text data."
      ],
      "metadata": {
        "id": "uZRF7DC9baMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dirichlet distribution\n",
        "\n",
        "The Dirichlet distribution is a probability distribution that's often used in statistics and machine learning, particularly in the context of Bayesian inference and topic modeling like Latent Dirichlet Allocation (LDA). It's named after the mathematician Peter Gustav Lejeune Dirichlet.\n",
        "\n",
        "Here's a simplified explanation of the Dirichlet distribution:\n",
        "\n",
        "1. **Multinomial Distribution**:\n",
        "   - To understand the Dirichlet distribution, it's helpful to know about the multinomial distribution first. The multinomial distribution models the probability of observing counts among multiple categories. For example, it can describe the probability of getting different outcomes when rolling a multi-sided die.\n",
        "\n",
        "2. **Extension to Multiple Categories**:\n",
        "   - The Dirichlet distribution is an extension of the multinomial distribution when you have more than two categories (often called \"multinomial with more than two outcomes\").\n",
        "   - Instead of modeling the probability of a single category like the multinomial, the Dirichlet distribution models the probabilities of multiple categories simultaneously.\n",
        "\n",
        "3. **Parameters**:\n",
        "   - The Dirichlet distribution has parameters that affect the shape of the distribution. These parameters are often denoted as α (alpha).\n",
        "   - α is a vector where each element corresponds to a category or topic. It determines the concentration of probability mass on each category. Higher α values mean more concentration, while lower values mean more uniformity.\n",
        "\n",
        "4. **Applications**:\n",
        "   - It's used in situations where you want to model the probability distribution over a set of categorical variables, such as the likelihood of different topics in a document (as in LDA) or the distribution of words in topics.\n",
        "\n",
        "\n",
        "5. **In LDA**:\n",
        "   - In the context of LDA, the Dirichlet distribution is used to model the distribution of topics in documents and the distribution of words in topics.\n",
        "   - The Dirichlet distribution helps LDA figure out how topics are distributed across documents and how words are distributed across topics. It does this by adjusting the probabilities based on observed data.\n",
        "   - Image analysis: It's used in color modeling and segmentation tasks.\n",
        "   - Genetics: It's used to model allele frequencies in populations.\n",
        "\n",
        "In summary, the Dirichlet distribution is a fundamental tool in probability theory and Bayesian statistics, and it's especially important in topic modeling to model the distributions of topics and words in a corpus of documents. It allows us to capture the uncertainty and relationships between categories or topics in a principled way."
      ],
      "metadata": {
        "id": "hhFED0T8al1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Dirichlet distribution -  Role in Latent Dirichlet Allocation (LDA)\n",
        " The Dirichlet distribution plays a crucial role in Latent Dirichlet Allocation (LDA), a topic modeling technique. In LDA, the Dirichlet distribution is used to model the distribution of topics within documents and the distribution of words within topics. Here's an example of how the Dirichlet distribution is applied in LDA:\n",
        "\n",
        "**Scenario: Topic Modeling in a Collection of News Articles**\n",
        "\n",
        "Let's say you have a collection of news articles, and you want to discover the main topics within these articles using LDA.\n",
        "\n",
        "1. **Document-Topic Distribution (θ)**:\n",
        "   - LDA uses the Dirichlet distribution to model the distribution of topics within each document. Each document has a probability distribution over topics.\n",
        "   - In our case, let's consider three topics: \"politics,\" \"technology,\" and \"sports.\"\n",
        "\n",
        "2. **Dirichlet Parameters (α)**:\n",
        "   - You set the Dirichlet parameters (α) to control the concentration of topics within documents. Higher α values mean documents are more likely to contain a mix of topics, while lower values make documents more focused on a single topic.\n",
        "   - You choose α = [1, 1, 1] to represent a scenario where documents are evenly mixed with topics.\n",
        "\n",
        "3. **Topic-Word Distribution (β)**:\n",
        "   - LDA also uses the Dirichlet distribution to model the distribution of words within topics. Each topic has a probability distribution over words.\n",
        "   - Let's consider a vocabulary of words relevant to the topics.\n",
        "\n",
        "4. **Dirichlet Parameters (η)**:\n",
        "   - You set the Dirichlet parameters (η) to control the concentration of words within topics. Higher η values mean topics are more likely to include a wide range of words, while lower values make topics more focused on specific words.\n",
        "   - You choose η = [0.1, 0.1, 0.1] to represent a scenario where topics are diverse in their word usage.\n",
        "\n",
        "5. **LDA Model Training**:\n",
        "   - Using these parameters, LDA iteratively trains the model to fit the Dirichlet distributions to the documents and words.\n",
        "   - As a result, you obtain two sets of probability distributions: one representing how topics are distributed within each document (θ), and another representing how words are distributed within each topic (β).\n",
        "\n",
        "6. **Interpreting Results**:\n",
        "   - With the trained LDA model, you can now interpret the results. Each document has a distribution over topics, and each topic has a distribution over words.\n",
        "   - For instance, you might find that a particular document has a high probability for the \"politics\" topic and contains words like \"election,\" \"candidate,\" and \"policy.\"\n",
        "\n",
        "By applying the Dirichlet distribution within LDA, you can uncover the underlying topics within a collection of documents and understand how words are associated with these topics. It allows you to perform automated topic modeling and discover the main themes or subjects discussed in the news articles."
      ],
      "metadata": {
        "id": "yzAb4-c9KVfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Latent Dirichlet Allocation (LDA) through example:"
      ],
      "metadata": {
        "id": "KBEknkYjceQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "cJupSz0qKwmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_dWH_qeKlIe",
        "outputId": "daef8a50-512e-4d51-8c53-4ae2cbd1353d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data collection"
      ],
      "metadata": {
        "id": "T_UWNS35K1w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample collection of news articles\n",
        "news_articles = [\n",
        "    \"Political candidates debate important issues in the election.\",\n",
        "    \"The new smartphone technology is revolutionizing the market.\",\n",
        "    \"The baseball team won the championship game.\",\n",
        "    \"Government policies are affecting the economy.\",\n",
        "    \"Tech companies are investing in artificial intelligence.\",\n",
        "]"
      ],
      "metadata": {
        "id": "4ReYq0zVKt6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "Ade3t3ksK8Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: Tokenize, remove stopwords, and create a dictionary\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokenized_articles = [word_tokenize(article.lower()) for article in news_articles]\n",
        "filtered_articles = [[word for word in article if word.isalnum() and word not in stop_words] for article in tokenized_articles]"
      ],
      "metadata": {
        "id": "dKGuRbhcKt22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dictionary:\n",
        "\n",
        "In this step, we create a dictionary from the preprocessed news articles (filtered_articles).\n",
        "The corpora.Dictionary function from the gensim library is used to create a mapping between unique words (terms) and their integer IDs. Each word in the articles is assigned a unique ID."
      ],
      "metadata": {
        "id": "fyMo9e4DLAKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(filtered_articles)"
      ],
      "metadata": {
        "id": "mKEsjkU6LI9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Document-Term Matrix (Corpus):\n",
        "\n",
        "Here, we create a document-term matrix, often referred to as the \"corpus.\" This matrix represents how many times each word appears in each document.\n",
        "The dictionary.doc2bow method converts each preprocessed article (a list of words) into a bag-of-words representation. It counts the frequency of each word and maps them to their corresponding IDs in the dictionary.\n",
        "The result is a list of tuples for each document, where each tuple contains a word ID and its frequency in that document."
      ],
      "metadata": {
        "id": "OKz2eCMNLPTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(article) for article in filtered_articles]"
      ],
      "metadata": {
        "id": "XATQn-JBLRNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the LDA Model:\n",
        "\n",
        "In this step, we train the LDA model using the corpus created in the previous step.\n",
        "We specify the number of topics we want to extract from the articles using num_topics. In this example, we choose to extract 3 topics.\n",
        "The LdaModel function from gensim.models is used to create and train the LDA model. It takes the corpus (corpus), the specified number of topics (num_topics), the dictionary (dictionary), and the number of passes (iterations) through the corpus (passes) as parameters.\n",
        "After training, lda_model will contain information about the topics discovered in the articles and the distribution of words within those topics."
      ],
      "metadata": {
        "id": "DqLYS2FhLZWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 5  # Specify the number of topics\n",
        "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)"
      ],
      "metadata": {
        "id": "NIyA1H45LdD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Topics and assosiated words\n",
        "\n",
        "After training, extract the topics and the top words associated with each topic. This allows you to interpret and label the topics effectively."
      ],
      "metadata": {
        "id": "XGdy4my1Lf3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the topics and their associated words\n",
        "for topic_id, topic_words in lda_model.print_topics():\n",
        "    print(f\"Topic {topic_id}: {topic_words}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4XA22tNKtun",
        "outputId": "411ba566-d3a9-459e-d4fd-ca7174f3370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.122*\"new\" + 0.122*\"smartphone\" + 0.122*\"revolutionizing\" + 0.122*\"technology\" + 0.122*\"market\" + 0.020*\"team\" + 0.020*\"baseball\" + 0.020*\"championship\" + 0.020*\"game\" + 0.020*\"intelligence\"\n",
            "\n",
            "Topic 1: 0.087*\"investing\" + 0.087*\"companies\" + 0.087*\"tech\" + 0.087*\"artificial\" + 0.087*\"intelligence\" + 0.087*\"game\" + 0.087*\"championship\" + 0.087*\"baseball\" + 0.087*\"team\" + 0.014*\"technology\"\n",
            "\n",
            "Topic 2: 0.111*\"debate\" + 0.111*\"candidates\" + 0.111*\"election\" + 0.111*\"issues\" + 0.111*\"political\" + 0.111*\"important\" + 0.019*\"team\" + 0.019*\"intelligence\" + 0.019*\"baseball\" + 0.019*\"championship\"\n",
            "\n",
            "Topic 3: 0.042*\"team\" + 0.042*\"baseball\" + 0.042*\"championship\" + 0.042*\"game\" + 0.042*\"artificial\" + 0.042*\"intelligence\" + 0.042*\"tech\" + 0.042*\"affecting\" + 0.042*\"companies\" + 0.042*\"technology\"\n",
            "\n",
            "Topic 4: 0.136*\"economy\" + 0.136*\"policies\" + 0.136*\"government\" + 0.136*\"affecting\" + 0.023*\"team\" + 0.023*\"baseball\" + 0.023*\"championship\" + 0.023*\"tech\" + 0.023*\"game\" + 0.023*\"investing\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assigning Lables and their interpretations for the LDA topics\n",
        "\n",
        "\n",
        "**Topic 0: Technological Innovation**\n",
        "\n",
        "Words: \"new,\" \"smartphone,\" \"revolutionizing,\" \"technology,\" \"market,\" \"team,\" \"baseball,\" \"championship,\" \"game,\" \"intelligence\"\n",
        "Interpretation: This topic focuses on technological innovation and its impact on the market. It discusses new developments in smartphones and technology that are revolutionizing the market. The references to \"team,\" \"baseball,\" and \"championship\" might indicate a connection to sports-related technology advancements.\n",
        "\n",
        "**Topic 1: Tech Investment and Artificial Intelligence**\n",
        "\n",
        "Words: \"investing,\" \"companies,\" \"tech,\" \"artificial,\" \"intelligence,\" \"game,\" \"championship,\" \"baseball,\" \"team,\" \"technology\"\n",
        "Interpretation: This topic appears to be related to investment in tech companies and the role of artificial intelligence. It discusses investments in technology companies and developments in artificial intelligence. The sports-related terms may not be directly related to the topic.\n",
        "\n",
        "**Topic 2: Political Debates and Elections**\n",
        "\n",
        "Words: \"debate,\" \"candidates,\" \"election,\" \"issues,\" \"political,\" \"important,\" \"team,\" \"intelligence,\" \"baseball,\" \"championship\"\n",
        "Interpretation: This topic revolves around political debates, election candidates, and important political issues. It discusses topics related to elections and political debates. The sports-related terms may be coincidental.\n",
        "\n",
        "**Topic 3: Sports and Technology**\n",
        "\n",
        "Words: \"team,\" \"baseball,\" \"championship,\" \"game,\" \"artificial,\" \"intelligence,\" \"tech,\" \"affecting,\" \"companies,\" \"technology\"\n",
        "Interpretation: This topic combines discussions about sports, technology, and its impact on sports-related topics. It may discuss how technology affects sports teams, championships, and games. The presence of \"artificial intelligence\" suggests a connection between technology and sports.\n",
        "\n",
        "**Topic 4: Government Policies and Economic Impact**\n",
        "\n",
        "Words: \"economy,\" \"policies,\" \"government,\" \"affecting,\" \"team,\" \"baseball,\" \"championship,\" \"tech,\" \"game,\" \"investing\"\n",
        "Interpretation: This topic is likely related to government policies and their impact on the economy. It discusses how government decisions affect various aspects, potentially including sports and technology. The sports-related terms may not be directly related to the core topic."
      ],
      "metadata": {
        "id": "0WgmGcFgfr7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation:\n",
        "\n",
        "Assess the quality of your LDA model using evaluation metrics like coherence scores. Higher coherence scores often indicate better topic quality.\n",
        "\n",
        "## Coherence Score\n",
        "\n",
        "The coherence score is a measure of the quality and interpretability of topics generated by an LDA (Latent Dirichlet Allocation) model. It quantifies how closely related the words within each topic are and how distinct the topics are from each other.\n"
      ],
      "metadata": {
        "id": "YYL4mmQaLqSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate coherence score to evaluate model quality\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=filtered_articles, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbLbMpJVLs-W",
        "outputId": "7cc009de-8f95-499a-a752-b68484128f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 0.5355969851009345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretation :\n",
        "- Coherence scores typically range between 0 and 1. A higher coherence score suggests better-defined and more interpretable topics.\n",
        "- A score around 0.5 is moderate and indicates that the topics generated by your LDA model have some level of coherence but could potentially be improved for better interpretability.\n",
        "- A score below 0.5 would suggest that the topics are less coherent and may not provide clear insights.\n"
      ],
      "metadata": {
        "id": "7j5QHB2Bkbzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary:\n",
        "The Dirichlet distribution plays a crucial role in Latent Dirichlet Allocation (LDA), a popular topic modeling technique used for uncovering hidden topics within a collection of documents. LDA aims to discover the underlying topics in a corpus and their associated word distributions. The Dirichlet distribution shapes the probability distributions in LDA, influencing how topics and words are modeled.\n",
        "\n",
        "In the presented example, we demonstrated the steps involved in LDA, from importing libraries to evaluating model quality. We collected a sample of news articles, preprocessed the text data by tokenizing and removing stopwords, created a dictionary and a document-term matrix (corpus), and trained the LDA model to generate topics and their associated words. We used a coherence score to evaluate the model's quality, with a higher score indicating more coherent topics.\n",
        "\n",
        "# Conclusion:\n",
        "Understanding the Dirichlet distribution is fundamental for grasping the inner workings of LDA . **The Dirichlet distribution helps define how topics are distributed in documents and how words are distributed within topics**. It serves as a key component in the probabilistic framework of LDA, enabling the model to uncover meaningful topics from unstructured text data.\n",
        "\n",
        "By following the outlined steps and utilizing the Dirichlet distribution, we can gain valuable insights from textual data, whether it's for topic modeling, document clustering, or other natural language processing tasks. Moreover, the coherence score provides a quantitative measure of topic quality, assisting in model evaluation and refinement.\n",
        "\n",
        "In conclusion, the Dirichlet distribution empowers LDA to extract hidden topics from text, making it a valuable tool for uncovering the latent structure of large document collections."
      ],
      "metadata": {
        "id": "P-vW6DQ-pPXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thank you for reading till the end."
      ],
      "metadata": {
        "id": "LQaXjIPip7HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## -Raviteja"
      ],
      "metadata": {
        "id": "fNJ9_xUvp-wp"
      }
    }
  ]
}